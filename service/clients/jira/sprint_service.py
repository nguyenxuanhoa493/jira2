from narwhals import dataframe
from pandas.tseries.frequencies import key
from service.base.jira_base import JiraBase
import streamlit as st
import pandas as pd
from conf import (
    DEFAULT_FIELDS_ISSUE,
    KEY_ISSUE_DEBUG,
    DEFAULT_MAX_ISSUE_PER_PAGE,
    STATUS_ORDER,
)
from service.utils.time_utils import (
    cal_hours_since_update,
    convert_time_str_to_datetime,
    convert_seconds_to_jira_time,
)
from service.clients.jira.worklog_service import WorklogService
from service.utils.date_utils import parse_jira_datetime
from service.utils.cache_utils import file_cache
from typing import Optional
from datetime import datetime
from service.utils.date_utils import adjust_sprint_dates


class SprintService(JiraBase):
    start_date = None
    end_date = None
    board_id = 0

    """Service qu·∫£n l√Ω sprint trong Jira"""

    def __init__(self, board_id=None, data_sprint: dict = {}):
        super().__init__()
        if board_id:
            self.board_id = board_id

    def set_data_sprint(self, data_sprint: dict):
        self.board_id = data_sprint.get("originBoardId", None)
        self.start_date = data_sprint.get("startDate", None)
        self.end_date = data_sprint.get("endDate", None)
        self.start_date, self.end_date = adjust_sprint_dates(
            self.start_date, self.end_date
        )
        self.goal = data_sprint.get("goal", "")

    def set_board_id(self, board_id):
        """Thi·∫øt l·∫≠p board_id cho service"""
        self.board_id = board_id

    def set_time_range(self, start_date, end_date):
        self.start_date = start_date
        self.end_date = end_date

    def get_list_sprints(self, state: str = "", sort_by_state: bool = True):
        """
        L·∫•y danh s√°ch sprint c·ªßa 1 board.
        state: 'active', 'future', 'closed' ho·∫∑c None
        """
        if not self.board_id:
            raise ValueError("Board ID ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p")

        params = {}
        if state:
            params["state"] = state
        list_sprint = self.jira._get_json(
            f"board/{self.board_id}/sprint",
            params,
            base=self.jira.AGILE_BASE_URL,
        ).get("values", [])

        filtered_sprints = [
            sprint
            for sprint in list_sprint
            if sprint.get("originBoardId") == self.board_id
        ][::-1]

        for idx, sprint in enumerate(filtered_sprints):
            filtered_sprints[idx][
                "name"
            ] = f"{sprint['name']} - {sprint['state'].upper()}"

        # S·∫Øp x·∫øp theo th·ª© t·ª± state: active, future, closed
        state_order = {"active": 0, "future": 1, "closed": 2}

        def sort_key(sprint):
            sprint_state = sprint.get("state", "").lower()
            return state_order.get(sprint_state, 3)  # C√°c state kh√°c s·∫Ω ƒë∆∞·ª£c ƒë·∫∑t cu·ªëi

        return (
            sorted(filtered_sprints, key=sort_key)
            if sort_by_state
            else filtered_sprints
        )

    def get_issues_for_sprint(
        self,
        sprint_id: int,
        fields: list = DEFAULT_FIELDS_ISSUE,
        max_results: int = DEFAULT_MAX_ISSUE_PER_PAGE,
        use_cache: bool = True,
        return_cache_info: bool = False,
    ):
        """
        L·∫•y issues cho sprint v·ªõi file cache (persistent - kh√¥ng expire t·ª± ƒë·ªông)

        Args:
            sprint_id: ID c·ªßa sprint
            fields: List c√°c field c·∫ßn l·∫•y
            max_results: S·ªë l∆∞·ª£ng t·ªëi ƒëa m·ªói page
            use_cache: C√≥ s·ª≠ d·ª•ng cache kh√¥ng
            return_cache_info: C√≥ tr·∫£ v·ªÅ th√¥ng tin cache kh√¥ng

        Returns:
            issues list ho·∫∑c tuple (issues, cache_info) n·∫øu return_cache_info=True
        """
        if not sprint_id:
            raise ValueError("Sprint ID is required")

        # T·∫°o cache key theo y√™u c·∫ßu: cache_issues_sprint + board_id
        cache_key = f"cache_issues_sprint_{self.board_id}_{sprint_id}"
        cache_info = {"from_cache": False, "timestamp": None}

        # Ki·ªÉm tra cache tr∆∞·ªõc n·∫øu use_cache = True
        if use_cache:
            cached_issues, cache_metadata = file_cache.load_cache_with_metadata(
                cache_key
            )
            if cached_issues is not None:
                st.toast("‚ö° Sprint issues loaded t·ª´ file cache")
                self.list_issues = pd.DataFrame(cached_issues)
                cache_info = {
                    "from_cache": True,
                    "timestamp": (
                        cache_metadata.get("timestamp") if cache_metadata else None
                    ),
                }

                if return_cache_info:
                    return cached_issues, cache_info
                return cached_issues

        # N·∫øu kh√¥ng c√≥ cache ho·∫∑c user ch·ªçn kh√¥ng d√πng cache, call API
        with st.spinner(f"üîÑ ƒêang load issues cho sprint {sprint_id}..."):
            all_issues = []
            start_at = 0
            while True:
                params = {
                    "fields": ",".join(fields),
                    "maxResults": max_results,
                    "startAt": start_at,
                    "expand": "changelog",
                }

                # S·ª≠ d·ª•ng endpoint ch√≠nh t·∫Øc ƒë·ªÉ l·∫•y issue t·ª´ sprint
                response = self.jira._get_json(
                    f"sprint/{sprint_id}/issue",
                    params,
                    base=self.jira.AGILE_BASE_URL,
                )
                issues_on_page = response.get("issues", [])
                if not issues_on_page:
                    break

                all_issues.extend(issues_on_page)

                total = response.get("total", 0)
                # break  # Only get one page
                if len(all_issues) >= total:
                    break

                start_at += len(issues_on_page)

            # X·ª≠ l√Ω v√† th√™m "points" v√†o m·ªói issue
            all_issues = [self._process_issue(issue) for issue in all_issues]

            # Cache k·∫øt qu·∫£ n·∫øu use_cache = True
            if use_cache and all_issues:
                file_cache.save_cache(cache_key, all_issues)
                st.toast(f"üíæ Sprint issues ƒë√£ ƒë∆∞·ª£c cache ({len(all_issues)} issues)")

        self.list_issues = pd.DataFrame(all_issues)

        if return_cache_info:
            return all_issues, cache_info
        return all_issues

    def clear_sprint_cache(self, sprint_id: int = 0):
        """
        X√≥a cache cho sprint c·ª• th·ªÉ ho·∫∑c t·∫•t c·∫£ sprint c·ªßa board

        Args:
            sprint_id: ID sprint c·∫ßn x√≥a cache, None ƒë·ªÉ x√≥a t·∫•t c·∫£
        """
        if sprint_id:
            cache_key = f"cache_issues_sprint_{self.board_id}_{sprint_id}"
            file_cache.clear_cache(cache_key)
            st.success(f"üóëÔ∏è ƒê√£ x√≥a cache cho sprint {sprint_id}")
        else:
            # X√≥a t·∫•t c·∫£ cache sprint c·ªßa board n√†y
            # V√¨ file_cache kh√¥ng support pattern matching,
            # ch√∫ng ta s·∫Ω ch·ªâ clear to√†n b·ªô cache
            file_cache.clear_cache()
            st.success(f"üóëÔ∏è ƒê√£ x√≥a t·∫•t c·∫£ cache")

    def get_cache_info(self):
        """L·∫•y th√¥ng tin cache hi·ªán t·∫°i"""
        return file_cache.get_cache_info()

    def cal_report_metric(self, issues):
        return issues

    def get_issue_active_in_sprint(self):
        return self.list_issues[self.list_issues["active_in_sprint"]]

    def _process_issue(
        self,
        issue,
    ):
        """X·ª≠ l√Ω d·ªØ li·ªáu cho m·ªôt issue, v√≠ d·ª• t√≠nh points."""
        worklog_service = WorklogService()
        # ƒê·∫£m b·∫£o issue c√≥ key 'fields'
        key = issue.get("key", "")
        # if key == "CLD-1060":

        #     st.write(key)
        #     st.write(issue)

        fields = issue.get("fields", {})
        summary = fields.get("summary", "")
        status = fields.get("status", {}).get("name", "")
        issuetype = fields.get("issuetype", {}).get("name", "")
        assignee = _get_field(fields, "assignee", "displayName")

        priority = fields.get("priority", {}).get("name", "")

        time_in_seconds = fields.get("timeoriginalestimate")
        points = round(time_in_seconds / 3600, 2) if time_in_seconds else 0.0
        tech = _get_field(data=fields, field_name="customfield_10192", key="value")
        is_development = _get_field(
            data=fields, field_name="customfield_10160", key="value", is_bool=True
        )
        is_popup = _get_field(
            data=fields, field_name="customfield_10130", key="value", is_bool=True
        )

        feature = _get_field(data=fields, field_name="customfield_10132", key="value")
        steve_estimate = _get_field(
            data=fields, field_name="customfield_10159", key="value"
        )
        customer = _get_field(data=fields, field_name="customfield_10092", key="value")
        has_subtasks = len(fields.get("subtasks", [])) > 0
        reporter = _get_field(data=fields, field_name="reporter", key="displayName")
        tester = _get_field(
            data=fields, field_name="customfield_10031", key="displayName"
        )
        env = _get_field(data=fields, field_name="customfield_10191", key="value")
        timetracking = fields.get("timetracking", {})
        remainingSeconds = timetracking.get(
            "originalEstimateSeconds", 0
        ) - timetracking.get("timeSpentSeconds", 0)
        remaining = convert_seconds_to_jira_time(remainingSeconds)
        duedate = convert_time_str_to_datetime(fields.get("duedate", ""))
        count_spinrt_closed = len(fields.get("closedSprints", []))
        updated_at = fields.get("updated", "")
        hours_since_update = cal_hours_since_update(updated_at)
        created_at = fields.get("created", "")
        worklogs = fields.get("worklog", {})
        worklog_list = worklogs.get("worklogs", [])
        # L·∫•y to√†n b·ªô worklog cho issue n·∫øu kh√¥ng ƒë·∫ßy ƒë·ªß
        if worklogs.get("total", 0) > len(worklog_list):
            st.toast(
                f"L·∫•y to√†n b·ªô worklog cho issue {key} v√¨ c√≥ {worklogs.get('total',0)} worklogs"
            )
            worklog_list = worklog_service.get_worklogs_by_issue_key(issue_key=key)

        # Ki·ªÉm tra v√† ƒë·∫£m b·∫£o start_date v√† end_date kh√¥ng None tr∆∞·ªõc khi g·ªçi
        if self.start_date is None or self.end_date is None:
            st.error("start_date ho·∫∑c end_date kh√¥ng ƒë∆∞·ª£c None")
            return None

        data_worklog = worklog_service.calculate_worklog_data(
            worklog_list,
            start_date=self.start_date,
            end_date=self.end_date,
        )

        changelog = self._process_changelog(
            issue.get("changelog", {}).get("histories", [])
        )
        is_show_dashboard = (
            True
            if (
                issuetype != "Epic" and is_development and assignee and not has_subtasks
            )
            else False
        )
        is_to_do = True if (status == "To Do" and is_show_dashboard) else False
        active_in_sprint = (
            True
            if (
                (
                    data_worklog["time_spent_in_sprint_seconds"]
                    and status != "Close"
                    and is_show_dashboard
                )
                or is_to_do
            )
            else False
        )

        if key == KEY_ISSUE_DEBUG and KEY_ISSUE_DEBUG:
            st.write(issue)

        issue_processed = {
            "key": key,
            "summary": summary,
            "status": status,
            "status_in_sprint": changelog["status_in_sprint"],
            "issuetype": issuetype,
            "assignee": assignee,
            "reporter": reporter,
            "tester": tester,
            "priority": priority,
            "points": points,
            "steve_estimate": steve_estimate,
            "tech": tech,
            "env": env,
            "customer": customer,
            "is_development": is_development,
            "is_show_dashboard": is_show_dashboard,
            "is_popup": is_popup,
            "feature": feature,
            "has_subtasks": has_subtasks,
            "active_in_sprint": active_in_sprint,
            "originalEstimate": timetracking.get("originalEstimate", "0h"),
            "timeSpent": timetracking.get("timeSpent", "0h"),
            "remaining": remaining,
            "remainingEstimate": timetracking.get("remainingEstimate", "0h"),
            "originalEstimateSeconds": timetracking.get("originalEstimateSeconds", 0),
            "timeSpentSeconds": timetracking.get("timeSpentSeconds", 0),
            "remainingEstimateSeconds": timetracking.get("remainingEstimateSeconds", 0),
            "remainingSeconds": remainingSeconds,
            "count_spinrt_closed": count_spinrt_closed,
            "created_at": created_at,
            "updated_at": updated_at,
            "hours_elapsed": hours_since_update.get("hours_elapsed", 0),
            "hours_elapsed_str": hours_since_update.get("hours_elapsed_str", "0h"),
            "date_elapsed": hours_since_update.get("date_elapsed", "0d 0h"),
            # D·ªØ li·ªáu worklog m·ªõi
            "count_worklog": data_worklog["count_worklog"],
            "time_spent_in_sprint_hours": data_worklog["time_spent_in_sprint_hours"],
            "time_spent_in_sprint_seconds": data_worklog[
                "time_spent_in_sprint_seconds"
            ],
            "unique_loggers_count": data_worklog["unique_loggers_count"],
            "first_time_in_progress": changelog["first_time_in_progress"],
            "count_reopen": changelog["count_reopen"],
            "has_reopen": changelog["has_reopen"],
            "reopen_in_sprint": changelog["reopen_in_sprint"],
            "is_done_in_sprint": changelog["is_done_in_sprint"],
            "time_done_in_sprint": changelog["time_done_in_sprint"],
            "duration_hours_to_done": changelog["duration_hours_to_done"],
            "duration_date_to_done": changelog["duration_date_to_done"],
            "duedate": duedate,
        }
        return issue_processed

    def print_list_issues(self, data):
        data = data or self.list_issues
        st.write(data)

    def _process_changelog(self, histories: list):
        status_changes = []
        count_reopen = 0

        is_done_in_sprint = False
        first_time_in_progress = None
        time_done_in_sprint = None
        reopen_in_sprint = False

        status_in_sprint = "To Do"

        # S·∫Øp x·∫øp l·ªãch s·ª≠ ƒë·ªÉ ƒë·∫£m b·∫£o x·ª≠ l√Ω ƒë√∫ng th·ª© t·ª± th·ªùi gian
        histories.sort(key=lambda x: x.get("created", "s"))

        for history in histories:
            items = history.get("items", [])
            if len(items) == 1 and items[0].get("field") == "status":
                status_to = items[0].get("toString", "")
                status_from = items[0].get("fromString", "")
                time_change = convert_time_str_to_datetime(
                    history.get("created", "")
                )  # type: ignore
                author = history.get("author", {}).get("displayName", "")

                is_in_sprint = (
                    time_change is not None
                    and self.start_date is not None
                    and self.end_date is not None
                    and time_change >= self.start_date
                    and time_change <= self.end_date
                )

                if (
                    time_change is not None
                    and self.end_date is not None
                    and time_change <= self.end_date
                ):
                    status_in_sprint = status_to

                if status_from == "Reopen":
                    count_reopen += 1
                    reopen_in_sprint = True if is_in_sprint else False

                if status_to == "Done" and is_in_sprint:
                    is_done_in_sprint = True
                    time_done_in_sprint = time_change

                status_changes.append(
                    {
                        "created": time_change,
                        "author": author,
                        "fromString": status_from,
                        "toString": status_to,
                    }
                )
        first_time_in_progress = (
            convert_time_str_to_datetime(status_changes[0]["created"])
            if status_changes
            else None
        )
        duration_hours_to_done = (
            (time_done_in_sprint - first_time_in_progress)
            if time_done_in_sprint and first_time_in_progress
            else None
        )
        duration_date_to_done = (
            round(duration_hours_to_done.total_seconds() / 3600 / 24, 2)
            if duration_hours_to_done
            else 0.0
        )

        return {
            "status_in_sprint": status_in_sprint,
            "first_time_in_progress": first_time_in_progress,
            "count_reopen": count_reopen,
            "has_reopen": count_reopen > 0,
            "histories": status_changes,
            "is_done_in_sprint": is_done_in_sprint,
            "time_done_in_sprint": time_done_in_sprint,
            "duration_hours_to_done": duration_hours_to_done,
            "duration_date_to_done": duration_date_to_done,
            "reopen_in_sprint": reopen_in_sprint,
        }

    def get_metric_sprint(self):
        list_issues_active = self.get_issue_active_in_sprint()
        all_issues = self.list_issues[self.list_issues["is_show_dashboard"]]
        count_issues = len(all_issues["is_development"])
        st.write("Th·ªëng k√™ theo status_in_sprint")
        metric_by_status_in_sprint = _get_metric_by_key(
            all_issues, "status_in_sprint", list(STATUS_ORDER.keys())
        )
        metric_by_status_issues_active_in_sprint = _get_metric_by_key(
            list_issues_active, "status_in_sprint", list(STATUS_ORDER.keys())
        )
        metric_by_type_issues = _get_metric_by_key(all_issues, "issuetype")
        metric_by_type_issues_active = _get_metric_by_key(
            list_issues_active, "issuetype"
        )

        metric_by_priority = _get_metric_by_key(all_issues, "priority")
        metric_by_priority_active = _get_metric_by_key(list_issues_active, "priority")

        metric_by_feature = _get_metric_by_key(all_issues, "feature")
        metric_by_feature_active = _get_metric_by_key(list_issues_active, "feature")

        data = {
            "count_issues": count_issues,
            "count_issues_active": len(list_issues_active),
            "metric_by_status": {
                "all": metric_by_status_in_sprint,
                "active": metric_by_status_issues_active_in_sprint,
            },
            "metric_by_type": {
                "all": metric_by_type_issues,
                "active": metric_by_type_issues_active,
            },
            "metric_by_priority": {
                "all": metric_by_priority,
                "active": metric_by_priority_active,
            },
            "metric_by_feature": {
                "all": metric_by_feature,
                "active": metric_by_feature_active,
            },
        }
        return data


def _get_field(data: dict, field_name: str, key: str = "value", is_bool: bool = False):
    data_field = data.get(field_name, {})
    if data_field:

        vaule_field = (
            data_field[0].get(key, "")
            if isinstance(data_field, list)
            else data_field.get(key, "")
        )
    else:
        vaule_field = ""
    if is_bool:
        return True if (vaule_field and vaule_field == "YES") else False
    return vaule_field


def _get_metric_by_key(all_issues, key_count: str, order_by_list: list = []):
    status_counts = all_issues[key_count].value_counts().to_dict()
    if not order_by_list:
        return status_counts
    # S·∫Øp x·∫øp theo th·ª© t·ª± ƒë·ªãnh s·∫µn v√† th√™m c√°c status c√≤n l·∫°i
    metric_by_key = {
        key: status_counts[key] for key in order_by_list if key in status_counts
    }
    # Th√™m c√°c status kh√¥ng c√≥ trong SATAUS_ORDER
    metric_by_key.update(
        {key: count for key, count in status_counts.items() if key not in metric_by_key}
    )
    return metric_by_key
